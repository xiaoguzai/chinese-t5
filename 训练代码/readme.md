## t5模型之中，标签左移还是右移动？

首先我们需要明确，当前标签预测的是下一个标签的概率，因为当前的标签需要生成概率去计算下一个标签的概率。

比如bert之中的标签位置关系

### bert之中的标签位置关系

[cls]我爱吃饭[sep]吃     饭     [sep]

​                       吃     饭   [sep]

这样学习的过程之中，能够使得[sep]训练的时候学习的是下一个字吃的概率，而吃在训练的时候学习的是下一个字饭的概率。

所以在这部分之中，t5模型的标签进行左移动，对准前面的一个标签内容

### t5模型之中的标签位置关系

接下来查看t5模型这部分的标签移动

**t5模型与bert模型进行生成的时候，有一个很大的区别，在于t5模型是原始的input和decoder的input分开输入，而bert模型训练的过程是原始的input和decoder的input拼接在一起使用，这里与两个模型的结构有关，t5模型调用的是完整的transformers结构，而bert模型调用的只有transformers中的encoder部分。**

这里还是以上面的原始文本为我爱吃饭，输出文本为吃饭作为案例。

t5模型的encoder输入：[cls]我爱吃饭[sep]，t5模型的decoder输入：吃饭[sep]

这样计算loss的时候，在decoder的输入部分，需要将decoder的部分右移动一格

吃饭[sep]->[cls]吃饭

这样移动之后，结果为

encoder_input: [cls]我爱吃饭[sep]  

decoder_input和decoder_output: 

| [cls] | 吃   | 饭    |
| ----- | ---- | ----- |
| 吃    | 饭   | [sep] |

这样计算的时候本质上跟上面的bert训练的标签对齐方式一样，只不过这里的标签不一样，所以变成输入右移动了。

### 做一个小小的变换可以统一为输出左移的方法

上面显示的是transformers库中t5模型原始的训练过程，

**所以这里实际上是输入的部分小小的变换一下，即可都改为标签左移**

输入变成[cls]吃饭，则输出的时候对输入右移一下

[cls]吃饭->吃饭[sep]

encoder_input:[cls]我爱吃饭  

decoder_input和decoder_output:

| [cls] | 吃   | 饭    |
| ----- | ---- | ----- |
| 吃    | 饭   | [sep] |

这个时候bert的输出左移就和decoder的输出左移统一了

**左移还是右移本身不重要，关键在于当前标签要能够对齐下一个标签，这样才能够方便生成。**

​                        